**Learning Transferable Visual Models From Natural Language Supervision**

- **背景**
  - 基于纯文本预训练+零样本适配下游任务的统一语言模型推动了NLP进入任务无关，零样本迁移的新时代
  - 在NLP领域 大数据弱监督>小数据强监督
  - 在CV领域预训练仍然高度依赖人工标注
- **现有问题**
  - 传统的计算机视觉模型（分类器）只能识别固定的一组标签
  - 现有模型的数据和计算资源规模不够
    - 现有多模态模型大多使用三类图文匹配对数据集
      - MS-COCO
      - Visual Genome
      - YFCC100M
- **动机**
  - 利用自然语言监督视觉模型训练的视觉网络，将弱监督+对比学习+大规模图文数据应用于CV
- **贡献**
  - 利用自然语言监督信号定义全新的分类器，实现Zero-Shot学习
- **解决思路**
  - **使用文本来监督图像学习，而不是提前标注好的分类**
  - **使用对比学习损失**
    - 使用VirTex根据图像生成文本，再让生成的文本对齐初始文本的方法成本太高
    - 受对比学习的影响，作者放弃逐字预测的caption任务，转而只判断一对图文是否匹配
    - 替换后效率提高了四倍
  - **从头训练，不使用预训练权重**
  - **只保留线性投影**
  - **简单的数据增强，随机缩放再裁剪**
  - **softmax温度参数设为可学习**
- **具体解决方法**
- **实验**
  - **数据集**
    - **WIT(Web Image Text)**
      - 四亿条图文对，来源于多个互联网公开渠道
      - 涵盖了50W个物体类别，每个类别最多保留两万条图文对
      - 数据集总词数和GPT-2使用的WebText相当
  - **模型配置**
    - **文本编码器**
      - 12层
      - 512dim
      - 8头
    - **图像编码器**
      - **ResNet系列**
        - ResNet50
        - ResNet101
        - RN50x4: ResNet50 4倍计算量
        - RN50x16: ResNet50 14倍计算量
        - RN50x64: ResNet50 64倍计算量
      - **ViT系列**
        - ViT-B/32：Base 模型，Patch size 为 32×32
        - ViT-B/16：Base 模型，Patch size 为 16×16，分辨率更高
        - ViT-L/14：Large 模型，Patch size 为 14×14，模型规模更大
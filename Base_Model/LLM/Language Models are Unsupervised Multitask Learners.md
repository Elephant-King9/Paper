**Language Models are Unsupervised Multitask Learners**

- **背景**

- **现有问题**

- **动机**
  - 在Transformer Decoder的基础上扩大模型规模
  - 更加关注zero-shot
  
- **贡献**

- **解决思路**
  - **预训练+zero-shot**
    - **zero-shot**
      - 在适配下游的时候模型参数不能被调整了
        - **引入prompt**
  
- **具体解决办法**

- **实验**

  - **数据集**
    - **WebText**
      - **使用Reddit过滤的Common Crawl**
      - **8million文本(40GB文字)**
      - 公开的网页抓取项目，爬虫抓取，脏数据较多
      - 使用Reddit网友过滤
    
  - **模型参数**
    - **GPT-2 117M**
      - **层数: 12**
      - **特征维度: 768**
      - **参数量: 117M**
    - **GPT-2 345M**
      - **层数: 24**
      - **特征维度: 1024**
      - **参数量: 345M**
    - **GPT-2 762M**
      - **层数: 36**
      - **特征维度: 1280**
      - **参数量: 762M**
    - **GPT-2 1524M**
      - **层数: 48**
      - **特征维度: 1600**
      - **参数量: 1524M**
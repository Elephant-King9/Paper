**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**

- **背景**
- **现有问题**
  - 认为GPT-1使用Decoder单向编码会存在全局语义的盲区
- **动机**
- **贡献**
  - **展示了双向预训练对语言表示的重要性**
  - **一个统一模型+微调可以适应不同任务**
  - **在11项自然语言处理任务上实现SOTA**
- **解决思路**
- **具体解决办法**
- **实验**
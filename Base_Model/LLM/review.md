### 1. **(Decoder)Improving Language Understanding by Generative Pre-Training**

- **GPT-1**

- **作者:Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever**

- **OpenAI**

- **终版提交:2018.06**

- **Cite:13424**

- **现有问题:**NLP领域没有大量的标注好的数据，并且现有的优化损失不确定

- **创新点:**

  **==1.使用了无监督预训练+监督微调的办法适配下游任务==**

  **==2.使用了Transformer-decoder设计==**

  **==3.定义了无监督训练与监督微调不同的损失函数==**

- ![image-20250619143051540](./assets/pics/review/image-20250619143051540.png)

- [详细信息](./Improving Language Understanding by Generative Pre-Training.md)

### 2. (Encoder)BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

- **BERT**
- **作者: Jacob Devlin、Ming-Wei Chang、 KentonLee、Kristina Toutanova**
- **Google**
- **NAACL:2019**
- **终版提交: 2018.10**
- **Cite:133047**
- **现有问题:**
- **贡献:**
- **创新点:**
- [详细信息](./BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.md)

